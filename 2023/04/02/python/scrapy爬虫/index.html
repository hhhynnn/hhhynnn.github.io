<!DOCTYPE html><html lang="zh-Hans" theme-mode="auto"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>scrapy爬虫 | Zenith</title><script>var config = {"root":"/","search":{"preload":false,"activeHolder":"键入以继续","blurHolder":"数据检索","noResult":"无 $0 相关数据"},"code":{"codeInfo":"$0 - 共 $1 行","copy":"复制","copyFinish":"复制成功","expand":"展开"}}</script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script src="//unpkg.com/mermaid@9.2.2/dist/mermaid.min.js"></script><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link rel="stylesheet" href="/css/arknights.css"><script>if (window.localStorage.getItem('theme-mode') === 'light' || window.matchMedia('(prefers-color-scheme:light)').matches) document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark' || window.matchMedia('(prefers-color-scheme:dark)').matches) document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Zenith" type="application/atom+xml">
</head><body><div class="loading" style="opacity: 0"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><nav><div class="navBtn hide"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="数据检索" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem"><a class="navBlock" href="/about/"><span class="navItemTitle">About</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Archives</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>scrapy爬虫</h1><div id="post-info"><span>文章发布时间: <div class="control"><time datetime="2023-04-02T07:45:50.000Z" id="date"> 2023-04-02</time></div></span><br><span>最后更新时间: <div class="control"><time datetime="2023-04-03T01:38:05.152Z" id="updated"> 2023-04-03</time></div></span></div></div><hr><div id="post-content"><p>几乎不怎么用 scrapy，遇到个爬取百度百科的任务，简单记录一下</p>
<span id="more"></span>

<h2 id="一、新建-scrapy-项目"><a href="#一、新建-scrapy-项目" class="headerlink" title="一、新建 scrapy 项目"></a>一、新建 scrapy 项目</h2><p>可以用代码直接创建模板化的 scrapy 项目，然后简单修改下直接启动</p>
<h3 id="1-创建并修改模板"><a href="#1-创建并修改模板" class="headerlink" title="1. 创建并修改模板"></a>1. 创建并修改模板</h3><p><strong>创建文件夹</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy startproject &lt;projectName&gt;</span><br></code></pre></td></tr></table></figure>

<p><strong>修改 <code>settings.py</code></strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">不遵守爬虫协议</span><br>ROBOTSTXT_OBEY=False<br></code></pre></td></tr></table></figure>

<p><strong>添加日志相关设置 <code>setting.py</code></strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">是否启用日志</span><br>LOG_ENABLED=True<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">日志使用的编码</span><br>LOG_ENCODING=&#x27;utf-8&#x27;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">日志文件(文件名)</span><br>LOG_FILE=None<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">日志格式</span><br>LOG_FORMAT=&#x27;%(asctime)s [%(name)s] %(levelname)s: %(message)s&#x27;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">日志时间格式</span><br>LOG_DATEFORMAT=&#x27;%Y-%m-%d %H:%M:%S&#x27;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">日志级别 CRITICAL, ERROR, WARNING, INFO, DEBUG</span><br>LOG_LEVEL=&#x27;DEBUG&#x27;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">如果等于True，所有的标准输出（包括错误）都会重定向到日志，例如：<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;hello&#x27;</span>)</span><br>LOG_STDOUT=False<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">如果等于True，日志仅仅包含根路径，False显示日志输出组件</span><br>LOG_SHORT_NAMES=False<br></code></pre></td></tr></table></figure>

<p>新建 spider</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</span><br></code></pre></td></tr></table></figure>

<h3 id="2-启动爬虫"><a href="#2-启动爬虫" class="headerlink" title="2. 启动爬虫"></a>2. 启动爬虫</h3><p>两种方法启动，命令行或者编写 .py 文件辅助执行命令行</p>
<p><strong>命令行启动</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">scrapy crawl &lt;spider_name&gt;</span><br></code></pre></td></tr></table></figure>

<p><strong>start.py 文件启动</strong></p>
<p>文件的存放位置如下图</p class='item-img' data-src='/2023/04/02/python/scrapy%E7%88%AC%E8%99%AB/image-20230403083931070.png'><img src="/2023/04/02/python/scrapy%E7%88%AC%E8%99%AB/image-20230403083931070.png" alt="image-20230403083931070" style="zoom:80%;">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># start.py</span><br><span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> cmdline<br>cmdline.execute(<span class="hljs-string">&quot;scrapy crawl opera_spider&quot;</span>.split())<br></code></pre></td></tr></table></figure>



<h2 id="二、登陆验证问题"><a href="#二、登陆验证问题" class="headerlink" title="二、登陆验证问题"></a>二、登陆验证问题</h2><p>有的网站需要提供 cookies, 或者 User Agent 之类的信息，可以参考下面的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ExampleSpider</span>(scrapy.Spider):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">start_requests</span>(<span class="hljs-params">self</span>):<br>        headers = &#123;<br>            <span class="hljs-string">&quot;Accept&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,<br>            <span class="hljs-string">&quot;Accept-Encoding&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,<br>            <span class="hljs-string">&quot;Accept-Language&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,<br>            <span class="hljs-string">&quot;Cookie&quot;</span>: <span class="hljs-string">&quot;&quot;</span><br>        &#125;<br>        <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> self.start_urls:<br>            <span class="hljs-keyword">yield</span> scrapy.FormRequest(url=url, callback=self.parse, headers=headers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>	<span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure>

<h3 id="添加-cookie"><a href="#添加-cookie" class="headerlink" title="添加 cookie"></a>添加 cookie</h3><p>主要有两种：</p>
<ol>
<li><p>直接加到 headers 里（如上面的代码），需要设置 COOKIES_ENABLED = False</p>
</li>
<li><p>用 cookies=cookies 这个参数传递（如下代码），需要设置 COOKIES_ENABLED = True</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">start_requests</span>(<span class="hljs-params">self</span>):<br>        headers = &#123;<br>            <span class="hljs-string">&quot;Accept&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,<br>            <span class="hljs-string">&quot;Accept-Encoding&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,<br>            <span class="hljs-string">&quot;Accept-Language&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,<br>        &#125;<br>        cookies = &#123;<span class="hljs-string">&quot;key1&quot;</span>: <span class="hljs-string">&quot;val1&quot;</span>, <span class="hljs-string">&quot;key2&quot;</span>: <span class="hljs-string">&quot;val2&quot;</span>&#125;<br>        <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> self.start_urls:<br>            <span class="hljs-keyword">yield</span> scrapy.FormRequest(url=url, callback=self.parse, headers=headers, cookies=cookies)<br></code></pre></td></tr></table></figure>

<p>其中第二种方法可能需要拆分 cookie 的字符串为列表，一行代码解决：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_cookies_to_dict</span>(<span class="hljs-params">cookies</span>):<br>    cookies = <span class="hljs-built_in">dict</span>([l.split(<span class="hljs-string">&quot;=&quot;</span>, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> cookies.split(<span class="hljs-string">&quot;; &quot;</span>)])<br>    <span class="hljs-keyword">return</span> cookies<br></code></pre></td></tr></table></figure>





<hr>
<p>忽然发现我的工作可以不用爬这个百科，暂时不写了，等需要爬的时候再来 (2023.4.3 记)</p>
<div id="paginator"></div></div><div id="post-footer"><div id="pages"><div class="footer-link" style="width: 50%;text-align:right;border-right:1px #fe2 solid"><a href="/2023/04/02/python/requests%E7%88%AC%E8%99%AB/">← Next requests爬虫</a></div><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2023/04/02/python/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%90%8E%E5%8F%B0%E6%89%A7%E8%A1%8Cpython/">服务器后台执行python Prev →</a></div></div></div><div id="comments"><div class="selector"><button class="valine-sel"></button></div><div id="valine"></div></div></div><div class="bottom-btn"><div><a id="to-top" onClick="scrolls.scrolltop();" title="回到顶部" style="opacity: 0; display: none;">∧</a><a id="to-index" href="#toc-div" title="文章目录">≡</a><a id="color-mode" onClick="colorMode.change()" title="切换主题"></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/">Zenith</a></h1><div id="description"><p>风雪夜归人</p></div></div><div id="aside-block"><div id="toc-div"><h1>目录</h1><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%96%B0%E5%BB%BA-scrapy-%E9%A1%B9%E7%9B%AE"><span class="toc-number">1.</span> <span class="toc-text">一、新建 scrapy 项目</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%88%9B%E5%BB%BA%E5%B9%B6%E4%BF%AE%E6%94%B9%E6%A8%A1%E6%9D%BF"><span class="toc-number">1.1.</span> <span class="toc-text">1. 创建并修改模板</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%90%AF%E5%8A%A8%E7%88%AC%E8%99%AB"><span class="toc-number">1.2.</span> <span class="toc-text">2. 启动爬虫</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%99%BB%E9%99%86%E9%AA%8C%E8%AF%81%E9%97%AE%E9%A2%98"><span class="toc-number">2.</span> <span class="toc-text">二、登陆验证问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0-cookie"><span class="toc-number">2.1.</span> <span class="toc-text">添加 cookie</span></a></li></ol></li></ol></div></div><footer><nobr>构建自 <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> 使用主题 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr>主题作者 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas><script src="/js/search.js"></script><script src="/js/arknights.js"></script><script src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script src="/js/pjax.js"></script><script class="pjax-js">reset= () => {new Valine({
 el: '#valine'
 , appId: 'qSBWEWJW9wsbLiL0rFTjUHbl-gzGzoHsz'
 , appKey: '0yF0ErNCNXzYHEyzxzYspMXE' , placeholder: '此条评论委托企鹅物流发送'
 , path: window.location.pathname
});code.findCode();
document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script></body></html>